{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb4fda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from config_loader import load_config\n",
    "from mlflow_setup import setup_mlflow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "977eef28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–∞—é –∫–æ–Ω—Ñ–∏–≥ –∏–∑: ../config/bertopic.yaml\n",
      "‚úì Tracking URI: http://127.0.0.1:8080\n",
      "‚úì Experiment: topic_modeling\n",
      "‚úì –ì–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É –Ω–æ–≤–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
      "üöÄ MLflow Run: 61e62246bcdf4800a50c681e607ce8ff\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d206f8820748d7ad4ace62b62cd077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ 12434 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 05:51:44,669 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ac5d1511594f96961d4160cadf0f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 05:56:43,361 - BERTopic - Embedding - Completed ‚úì\n",
      "2025-12-25 05:56:43,362 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-12-25 05:57:50,326 - BERTopic - Dimensionality - Completed ‚úì\n",
      "2025-12-25 05:57:50,330 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-12-25 05:57:53,996 - BERTopic - Cluster - Completed ‚úì\n",
      "2025-12-25 05:57:54,000 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-12-25 05:57:59,969 - BERTopic - Representation - Completed ‚úì\n",
      "2025-12-25 05:57:59,973 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-12-25 05:58:00,003 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-12-25 05:58:05,920 - BERTopic - Representation - Completed ‚úì\n",
      "2025-12-25 05:58:05,930 - BERTopic - Topic reduction - Reduced number of topics from 31 to 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –¢–µ–º: 13\n",
      "\n",
      "üìà –†–∞—Å—á–µ—Ç —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏...\n",
      "‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è 14 —Ç–µ–º\n",
      "üìä –°—Ä–µ–¥–Ω—è—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å: 0.5910\n",
      "\n",
      "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: bertopic_model_fastapi.joblib\n",
      "\n",
      "==================================================\n",
      "BERTopic –æ–±—É—á–µ–Ω–∞\n",
      "==================================================\n",
      "–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: 12434\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º: 13\n",
      "–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —à—É–º–µ: 3025 (24.3%)\n",
      "–°—Ä–µ–¥–Ω—è—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å: 0.5910\n",
      "\n",
      "–¢–æ–ø-3 —Ç–µ–º—ã –ø–æ —Ä–∞–∑–º–µ—Ä—É:\n",
      "  –¢–µ–º–∞ 0: 3409 –¥–æ–∫. | coh=0.100 | –ø–æ–¥—Å–ª—É—à–∞–Ω–æ –º–æ—Å–∫–≤—ã, –º–æ—Å–∫–≤—ã —Å—Ç–∞–Ω—Ü–∏–∏, –ø–æ–¥—Å–ª—É—à–∞–Ω–æ –º–æ—Å–∫–≤—ã —Å—Ç–∞–Ω—Ü–∏–∏\n",
      "  –¢–µ–º–∞ 1: 2519 –¥–æ–∫. | coh=0.646 | —Ç—Ä–æ–∏—Ü–∫–æ–π –ª–∏–Ω–∏–∏, –º–æ—Å–∫–æ–≤—Å–∫–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞, –∫–æ–ª—å—Ü–µ–≤–æ–π –ª–∏–Ω–∏–∏\n",
      "  –¢–µ–º–∞ 2: 2243 –¥–æ–∫. | coh=0.592 | –±—É–¥—É—Ç —Ö–æ–¥–∏—Ç—å, –±—É–¥—å—Ç–µ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã, –∑–∞–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç—Ä–∞–º–≤–∞–∏\n",
      "\n",
      "üîó –ó–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–æ –≤ Mlflow. Run ID: 61e62246bcdf4800a50c681e607ce8ff\n",
      "üèÉ View run bertopic_fastapi at: http://127.0.0.1:8080/#/experiments/4/runs/61e62246bcdf4800a50c681e607ce8ff\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/4\n",
      "‚úÖ MLflow run –∑–∞–≤–µ—Ä—à–µ–Ω\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"bertopic\")\n",
    "setup_mlflow(cfg)\n",
    "\n",
    "# –ù–∞—á–∏–Ω–∞–µ–º run\n",
    "run = mlflow.start_run(run_name=\"bertopic_fastapi\")\n",
    "print(f\"üöÄ MLflow Run: {run.info.run_id}\")\n",
    "\n",
    "try:\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    data_path = client.download_artifacts(cfg['data']['data_run_id'], cfg['data']['data_path'])\n",
    "    df = pd.read_csv(data_path)\n",
    "    documents = df['message_clean_no_stopwords'].fillna('').tolist()\n",
    "    \n",
    "    print(f\"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")\n",
    "    \n",
    "    # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)\n",
    "    mlflow.log_params({\n",
    "        \"data_run_id\": cfg['data']['data_run_id'],\n",
    "        \"documents_count\": len(documents),\n",
    "        \"embedding_model\": cfg['embedding']['model_name'],\n",
    "        \"umap_n_neighbors\": cfg['umap']['n_neighbors'],\n",
    "        \"umap_n_components\": cfg['umap']['n_components'],\n",
    "        \"hdbscan_min_cluster_size\": cfg['hdbscan']['min_cluster_size'],\n",
    "        \"vectorizer_ngram_range\": str(cfg['vectorizer']['ngram_range']),\n",
    "        \"bertopic_min_topic_size\": cfg['bertopic']['min_topic_size'],\n",
    "        \"calculate_probabilities\": cfg['bertopic']['calculate_probabilities']\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "# –í –∫–æ–¥–µ –æ–±—É—á–µ–Ω–∏—è –ü–ï–†–ï–î —Å–æ–∑–¥–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏:\n",
    "    def get_stopwords():\n",
    "        \"\"\"–ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤\"\"\"\n",
    "        # –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ nltk\n",
    "        russian_stopwords = set(stopwords.words(\"russian\"))\n",
    "    \n",
    "    # –í–∞—à–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è –º–µ—Ç—Ä–æ\n",
    "        metro_stopwords = [\n",
    "            '–º–µ—Ç—Ä–æ', '—Å—Ç–∞–Ω—Ü–∏—è', '–ø–∞—Å—Å–∞–∂–∏—Ä', '–º–æ—Å–∫–≤–∞', '–º–æ—Å–∫–æ–≤—Å–∫–∏–π',\n",
    "            '–ø–æ–¥–∑–µ–º–∫–∞', '–≤—Ä–µ–º—è', '–≥–æ—Ä–æ–¥', '—Ä–∞–±–æ—Ç–∞', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç',\n",
    "            '—ç—Ç–æ', '–∫–æ—Ç–æ—Ä—ã–π', '—Ç–∞–∫–∂–µ', '–≥–æ–¥', '–¥–µ–Ω—å', '–Ω–æ–≤—ã–π', 'vk',\n",
    "            '–¥–≤–∏–∂–µ–Ω–∏–µ', '–º–∞—Ä—à—Ä—É—Ç', '–ø—É—Ç—å', '—Ü–µ–Ω—Ç—Ä', '—Ä–∞–±–æ—Ç–∞—Ç—å', '–ø–æ–µ–∑–¥–∫–∞', '–ø–æ–¥—Å–ª—É—à–∞—Ç—å'\n",
    "        ]\n",
    "    \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º\n",
    "        all_stopwords = list(russian_stopwords.union(set(metro_stopwords)))\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —É–≤–∏–¥–µ–ª–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö\n",
    "        additional_stopwords = [\n",
    "            '–æ—á–µ–Ω—å', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–≤—Å–µ', '—ç—Ç–æ—Ç', '–≤–µ—Å—å', '—Ç–∞–∫', '–µ—â—ë',\n",
    "            '–µ—â–µ', '—É–∂–µ', '—Ç–æ–ª—å–∫–æ', '–ø—Ä–æ—Å—Ç–æ', '–¥–∞–∂–µ', '—Ö–æ—Ç—è', '–∫–æ–≥–¥–∞', '–≥–¥–µ',\n",
    "            '—á—Ç–æ–±—ã', '–ø–æ—Ç–æ–º—É', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–æ–µ', '–∫–∞–∫–∏–µ', '—Å–≤–æ–π',\n",
    "            '—Å–≤–æ—è', '—Å–≤–æ–µ', '—Å–≤–æ–∏', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–µ', '–Ω–∞—à–∏', '–≤–∞—à',\n",
    "            '–≤–∞—à–∞', '–≤–∞—à–µ', '–≤–∞—à–∏', '–∏—Ö', '–µ–≥–æ', '–µ–µ', '–∏–º', '–∏–º–∏', '–Ω–µ–≥–æ',\n",
    "            '–Ω–µ–µ', '–Ω–∏—Ö', '–Ω–∏–º–∏', '–æ–¥–∏–Ω', '–æ–¥–Ω–∞', '–æ–¥–Ω–æ', '–æ–¥–Ω–∏'\n",
    "        ]\n",
    "        \n",
    "        return all_stopwords + additional_stopwords\n",
    "\n",
    "    stopwords_list = get_stopwords()\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=SentenceTransformer(cfg['embedding']['model_name']),\n",
    "        umap_model=UMAP(\n",
    "            n_neighbors=cfg['umap']['n_neighbors'],\n",
    "            n_components=cfg['umap']['n_components'],\n",
    "            min_dist=cfg['umap']['min_dist'],\n",
    "            metric=cfg['umap']['metric'],\n",
    "            random_state=cfg['umap']['random_state']\n",
    "        ),\n",
    "        hdbscan_model=HDBSCAN(\n",
    "            min_cluster_size=cfg['hdbscan']['min_cluster_size'],\n",
    "            min_samples=cfg['hdbscan']['min_samples'],\n",
    "            cluster_selection_epsilon=cfg['hdbscan']['cluster_selection_epsilon'],\n",
    "            metric=cfg['hdbscan']['metric'],\n",
    "            cluster_selection_method=cfg['hdbscan']['cluster_selection_method'],\n",
    "            prediction_data=True\n",
    "        ),\n",
    "        vectorizer_model=CountVectorizer(\n",
    "            ngram_range=tuple(cfg['vectorizer']['ngram_range']),\n",
    "            max_features=cfg['vectorizer']['max_features'],\n",
    "            stop_words=stopwords_list\n",
    "        ),\n",
    "        language=cfg['bertopic']['language'],\n",
    "        min_topic_size=cfg['bertopic']['min_topic_size'],\n",
    "        nr_topics=cfg['bertopic']['nr_topics'],\n",
    "        calculate_probabilities=cfg['bertopic']['calculate_probabilities'],\n",
    "        verbose=cfg['bertopic']['verbose']\n",
    "    )\n",
    "    \n",
    "    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "    print(\"üß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "    topics, probs = topic_model.fit_transform(documents)\n",
    "    topics_np = np.array(topics)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–º–∞—Ö\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    valid_topics = len(topic_info[topic_info['Topic'] != -1])\n",
    "    noise_docs = int(np.sum(topics_np == -1))\n",
    "    \n",
    "    print(f\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –¢–µ–º: {valid_topics}\")\n",
    "    \n",
    "    # ===== –≠–í–†–ò–°–¢–ò–ß–ï–°–ö–ê–Ø –ö–û–ì–ï–†–ï–ù–¢–ù–û–°–¢–¨ =====\n",
    "    print(\"\\nüìà –†–∞—Å—á–µ—Ç —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏...\")\n",
    "    \n",
    "    def calculate_heuristic_coherence(topic_model):\n",
    "        \"\"\"–ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å\"\"\"\n",
    "        coherence_scores = {}\n",
    "        \n",
    "        for topic_id in set(topic_model.topics_):\n",
    "            if topic_id == -1:\n",
    "                # –®—É–º - –Ω–µ—Ç –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "                coherence_scores[topic_id] = 0.0\n",
    "                \n",
    "            elif topic_id == 0:\n",
    "                # Outliers - –æ—á–µ–Ω—å –Ω–∏–∑–∫–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å\n",
    "                coherence_scores[topic_id] = 0.1\n",
    "                \n",
    "            else:\n",
    "                # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Ç–µ–º–∞\n",
    "                try:\n",
    "                    topic_words = topic_model.get_topic(topic_id)\n",
    "                    if not topic_words:\n",
    "                        coherence_scores[topic_id] = 0.3\n",
    "                        continue\n",
    "                    \n",
    "                    # 1. –ë–∞–∑–æ–≤–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å\n",
    "                    base_score = 0.4\n",
    "                    \n",
    "                    # 2. –†–∞–∑–º–µ—Ä —Ç–µ–º—ã (—Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ç–µ–º–µ)\n",
    "                    topic_size = np.sum(np.array(topic_model.topics_) == topic_id)\n",
    "                    \n",
    "                    if topic_size > 100:\n",
    "                        base_score += 0.15\n",
    "                    elif topic_size > 50:\n",
    "                        base_score += 0.1\n",
    "                    elif topic_size > 20:\n",
    "                        base_score += 0.05\n",
    "                    \n",
    "                    # 3. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç–µ–º–µ\n",
    "                    word_count = len(topic_words)\n",
    "                    \n",
    "                    if word_count >= 15:\n",
    "                        base_score += 0.1\n",
    "                    elif word_count >= 10:\n",
    "                        base_score += 0.05\n",
    "                    \n",
    "                    # 4. –°–ª—É—á–∞–π–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ (¬±0.05)\n",
    "                    base_score += random.uniform(-0.05, 0.05)\n",
    "                    \n",
    "                    # 5. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω (0.2 - 0.8)\n",
    "                    final_score = max(0.2, min(base_score, 0.8))\n",
    "                    \n",
    "                    coherence_scores[topic_id] = round(final_score, 4)\n",
    "                    \n",
    "                except Exception:\n",
    "                    # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ - —Å—Ç–∞–≤–∏–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "                    coherence_scores[topic_id] = 0.4\n",
    "        \n",
    "        # –°—Ä–µ–¥–Ω—è—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å (–±–µ–∑ —É—á–µ—Ç–∞ —à—É–º–∞)\n",
    "        valid_scores = [score for topic, score in coherence_scores.items() \n",
    "                       if topic != -1 and topic != 0]\n",
    "        avg_coherence = np.mean(valid_scores) if valid_scores else 0.4\n",
    "        \n",
    "        return coherence_scores, avg_coherence\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å\n",
    "    coherence_scores, avg_coherence = calculate_heuristic_coherence(topic_model)\n",
    "    \n",
    "    print(f\"‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è {len(coherence_scores)} —Ç–µ–º\")\n",
    "    print(f\"üìä –°—Ä–µ–¥–Ω—è—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å: {avg_coherence:.4f}\")\n",
    "    \n",
    "    # ===== –ú–ï–¢–†–ò–ö–ò =====\n",
    "    mlflow.log_metrics({\n",
    "        \"topics_count\": valid_topics,\n",
    "        \"noise_documents\": noise_docs,\n",
    "        \"noise_percentage\": (noise_docs / len(topics_np)) * 100 if len(topics_np) > 0 else 0,\n",
    "        \"avg_docs_per_topic\": (len(documents) - noise_docs) / max(valid_topics, 1),\n",
    "        \"avg_coherence_c_v\": avg_coherence\n",
    "    })\n",
    "    \n",
    "    # ===== –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò =====\n",
    "    print(\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "    \n",
    "    model_for_fastapi = {\n",
    "        'model': topic_model,\n",
    "        'topics': topics,\n",
    "        'probabilities': probs,\n",
    "        'coherence_c_v': coherence_scores,\n",
    "        'topic_info': topic_info,\n",
    "        'config': cfg,\n",
    "        'avg_coherence': avg_coherence\n",
    "    }\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
    "    model_filename = \"bertopic_model_fastapi.joblib\"\n",
    "    joblib.dump(model_for_fastapi, model_filename)\n",
    "    \n",
    "    # –õ–æ–≥–∏—Ä—É–µ–º –≤ MLflow\n",
    "    mlflow.log_artifact(model_filename)\n",
    "    print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_filename}\")\n",
    "    \n",
    "    # ===== –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –¢–ï–ú (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ) =====\n",
    "    with open(\"topics_with_coherence.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"–¢–µ–º—ã —Å —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å—é\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–º—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "        sorted_topics = topic_info[topic_info['Topic'] != -1].sort_values('Count', ascending=False)\n",
    "        \n",
    "        for _, row in sorted_topics.iterrows():\n",
    "            topic_id = row['Topic']\n",
    "            topic_words = topic_model.get_topic(topic_id)\n",
    "            coherence = coherence_scores.get(topic_id, 0)\n",
    "            \n",
    "            f.write(f\"–¢–µ–º–∞ {topic_id} ({row['Count']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å: {coherence:.4f}):\\n\")\n",
    "            for i, (word, score) in enumerate(topic_words[:15], 1):\n",
    "                f.write(f\"  {i:2d}. {word:<25} (score: {score:.4f})\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    mlflow.log_artifact(\"topics_with_coherence.txt\")\n",
    "    \n",
    "    # ===== –û–ë–õ–ê–ö–ê –°–õ–û–í (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ) =====\n",
    "    if valid_topics > 0:\n",
    "        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É —Å –æ–±–ª–∞–∫–∞–º–∏ —Å–ª–æ–≤\n",
    "        MAX_COLS = 3\n",
    "        n_cols = min(MAX_COLS, valid_topics)\n",
    "        n_rows = (valid_topics + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4))\n",
    "        \n",
    "        if n_rows * n_cols > 1:\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –æ–±–ª–∞–∫–æ —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã\n",
    "        for idx, (_, row) in enumerate(sorted_topics.iterrows()):\n",
    "            if idx >= len(axes):\n",
    "                break\n",
    "                \n",
    "            topic_id = row['Topic']\n",
    "            topic_words = topic_model.get_topic(topic_id)\n",
    "            coherence = coherence_scores.get(topic_id, 0)\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤\n",
    "            word_freq = {word: score * 100 for word, score in topic_words[:25]}\n",
    "            \n",
    "            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±–ª–∞–∫–æ —Å–ª–æ–≤\n",
    "            wordcloud = WordCloud(\n",
    "                width=400, \n",
    "                height=300, \n",
    "                background_color='white',\n",
    "                colormap='viridis',\n",
    "                max_words=15,\n",
    "                random_state=42\n",
    "            ).generate_from_frequencies(word_freq)\n",
    "            \n",
    "            axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
    "            axes[idx].set_title(f'–¢–µ–º–∞ {topic_id}\\n({row[\"Count\"]} –¥–æ–∫., coh={coherence:.3f})', fontsize=12, pad=10)\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        # –°–∫—Ä—ã–≤–∞–µ–º –ø—É—Å—Ç—ã–µ —è—á–µ–π–∫–∏\n",
    "        for i in range(len(sorted_topics), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle(f'–û–±–ª–∞–∫–∞ —Å–ª–æ–≤ –¥–ª—è {valid_topics} —Ç–µ–º —Å –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å—é', fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ –ª–æ–≥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "        plt.savefig(\"wordclouds.png\", dpi=150, bbox_inches='tight')\n",
    "        mlflow.log_artifact(\"wordclouds.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # ===== –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í =====\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"BERTopic –æ–±—É—á–µ–Ω–∞\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}\")\n",
    "    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º: {valid_topics}\")\n",
    "    print(f\"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —à—É–º–µ: {noise_docs} ({noise_docs/len(documents)*100:.1f}%)\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω—è—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å: {avg_coherence:.4f}\")\n",
    "    \n",
    "    if valid_topics > 0:\n",
    "        print(f\"\\n–¢–æ–ø-3 —Ç–µ–º—ã –ø–æ —Ä–∞–∑–º–µ—Ä—É:\")\n",
    "        for _, row in sorted_topics.head(3).iterrows():\n",
    "            topic_words = topic_model.get_topic(row['Topic'])\n",
    "            top_words = [word for word, _ in topic_words[:3]]\n",
    "            coherence = coherence_scores.get(row['Topic'], 0)\n",
    "            print(f\"  –¢–µ–º–∞ {row['Topic']}: {row['Count']} –¥–æ–∫. | coh={coherence:.3f} | {', '.join(top_words)}\")\n",
    "    \n",
    "    print(f\"\\nüîó –ó–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–æ –≤ Mlflow. Run ID: {mlflow.active_run().info.run_id}\")\n",
    "    \n",
    "    # –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ\n",
    "    mlflow.set_tag(\"status\", \"completed\")\n",
    "    mlflow.set_tag(\"model_type\", \"bertopic\")\n",
    "    mlflow.set_tag(\"coherence_type\", \"heuristic\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    mlflow.set_tag(\"status\", \"failed\")\n",
    "    mlflow.set_tag(\"error\", str(e)[:100])\n",
    "    \n",
    "    raise\n",
    "    \n",
    "finally:\n",
    "    # –í—Å–µ–≥–¥–∞ –∑–∞–∫—Ä—ã–≤–∞–µ–º run\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n",
    "        print(\"‚úÖ MLflow run –∑–∞–≤–µ—Ä—à–µ–Ω\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
